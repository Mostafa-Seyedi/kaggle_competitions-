# Kaggle Machine Learning Collection ğŸ†

This repository contains my solutions and experiments for various Kaggle competitions. The projects are organized by task type, focusing on end-to-end Machine Learning pipelinesâ€”from raw data to final predictions.

## ğŸ“‚ Repository Structure

I've organized my work into categories based on the nature of the problem:

* **/Classification**: Projects involving binary or multi-class targets (e.g., Titanic, Heart Disease, Fraud Detection).
* **/Regression**: Projects focused on predicting continuous values (e.g., House Prices, Sales Forecasting).
* **/Computer-Vision**: Image classification and object detection tasks.
* **/NLP**: Text classification, sentiment analysis, and NER.

---

## ğŸš€ Projects Summary

| Project Name | Category | Key Techniques | Status |
| :--- | :--- | :--- | :--- |
| [Titanic Survival](./Classification/titanic) | Classification | Random Forest | Completed |
| [Exam_Q](./Regression/Regression_pipeline_challenge_1) | Regression | Preprocessing | Pipeline | competed |
| [MNIST](./Classification/MNIST) | Pipeline | Completed |

---

## ğŸ› ï¸ Tools & Tech Stack

* **Languages:** Python
* **Libraries:** Pandas, NumPy, Scikit-Learn
* **Optimization:** Optuna (Hyperparameter Tuning)
* **Environment:** Jupyter Notebooks / Kaggle Kernels 

---

## ğŸ“ˆ Methodology

For every competition, I follow a systematic approach:
1. **EDA:** Visualizing distributions, correlations, and missing data.
2. **Preprocessing:** Handling outliers, encoding categorical variables, and scaling.
3. **Feature Engineering:** Creating new features to capture underlying patterns. (Sometimes)
4. **Modeling:** Training baseline models and moving toward complex ensembles.
5. **Evaluation:** Tuning models based on competition-specific metrics (LogLoss, RMSE, etc.).

---

## ğŸ“œ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
